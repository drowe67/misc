\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{tikz}
\begin{document}

\title{ML quantisation of Vocoder Features}
\maketitle

The goal is to achieve improved quantisation using ML techniques:
\begin{enumerate}
\item Lower bit rate for equivalent distortion.  
\item Increased robustness to channel errors that traditional modulation and FEC?
\end{enumerate}

Expectations.  There is quite a bit of information in a mel spaced log spectral vector, and it will take a finite amount of bits to quantise it.  ML won't change this. Where ML can help is better transforms and time prediction over large windows.  For example finding non-linear dependencies and correlations and reducing the dimension of the bottleneck required to be quantised.  Better transforms and time series prediction than possible with linear techniques. Combination of spectral information with pitch and voicing features will save some bits if they are correlated with the spectral information.  A multistage VQ will work better with a small dimension, as the residual errors tend towards gaussian.

However this will have it's limits for our use case where the quantisation window must be kept to around 100ms (including initial state information). ML isn't magic - it will take more information to quantise 8 x 10ms vectors than 1 x 10ms (but not 8 times as much).  One could start with a ML transform of 4 x 10ms vectors, quantise that as an initial state, then quantise the residual errors from each 10ms frame.

Is running the VQ training loop (e.g. VQVAE) inside the ML system worth it?  Several practical problems, such as the rate the VQ adapts compared to the rest of the network, managing EWMA updates that attempt to zero out codebook entries, and possibly poorer perf than traditional k-means.  Traditional VQ works pretty well and can extract some non-linear dependencies. The VQ could be simulated in the training loop using small amounts of AWGN injection to ensure the latent space is well behaved. Then, the VQ can be designed on the training data in the bottleneck. In general, use traditional DSP/quantisation where possible, leave ML to what it does best.  Perhaps hybrid training, an epoch each.

Where to these fit in:
\begin{enumerate}
\item Limit the amount of information through pre processing, e.g. compression, equalisation, dynamic range limiting.  This would reduce the information in the log spectral vectors.  Traditional DSP or ML? I feel this is where the biggest gains are.
\item Training the VQ to minimise the weighted linear error (at $K=20$ or via $F$ at $K=80$).  Can this be performed using traditional VQ training?  Would we use a low dim vector for the VQ?
\item Training a system that is robust to channel errors (or noise in an analog channel). How to get something close to MAP/or use LLRs and other info?  A latent vector that can handle a lot of "noise".
\item Ensuring we get good energy distribution (formant bandwidths) under quantisation.  Preserving bandwidth more important that frequency - can we include this weighting in loss function.
\end{enumerate}

\begin{enumerate}
\item Experiment 1: Build an autoencoder for 1 vector.  Try different architectures, e.g. FC and conv1D.  Determine bottleneck dimension for 1dB^2 distortion for 1 and 4 concatenated vectors.  Try MSE and weighted linear loss functions. Determine if we can get any dimensionality reduction, as this would make VQ much easier (assuming VQ latent space is well behaved.
\item Experiment 2: Build a VQVAE (VQ that trains in a ML network).  Compare distortion to kmeans for a range of VQ sizes.  Do they get the same performance?
\end{enumerate}

\end{document}
