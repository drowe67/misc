\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{epigraph}
\usepackage{float}

\begin{document}

\title{Unwrapping Codec 2 Data Manifolds}
\maketitle

\epigraph{Uncrumpling paper balls is what machine learning is all about: finding neat representations for complex, highly folded data manifolds}{Francois Chollet \cite{chollet2018deep}}

Speech coding can be defined as the art of representing speech using a small number of parameters that can be efficiently sent over a channel.

Codec 2 700C resamples the pitch dependent, time varying $L=F_s/(2F_0), L=20..80$ harmonic magnitude samples $\{20log_{10}(A_m)\}, m=1..L$ to a set of fixed length $K=20$ samples $\{B_k\},k=1..K$.  These are then vector quantised $\hat{B_m}=Q(B_m)$.  Harmonic phase is recovered using a minimum phase model. The recovered phase spectra and hence energy distribution across the pitch cycle is dependent on the magnitude spectra available at the receiver.

There is some evidence that the distribution of energy over a pitch cycle is important for speech perception \cite{rowe2023_ratek_study}. To adequately represent male speech, the formant energy must be spread in time.  As Codec 2 recover phase using a minimum phase model - this implies narrow formant bandwidths must be maintained \footnote{Other ways of spreading time domain energy are possible, for example generating phases through heuristics, non-minimum phase models, manipulation of phases, or time domain compression \cite{rowe2023_ratek_study}}.  Narrow formant bandwidths lead to long decay times for filters ``ringing`` with formant energy. When low resolution spectral sampling is employed (e.g. smoothed vector quantised spectra or low bit rate LSP quantisation of LPC models), male speech becomes buzzy, muffled and less intelligible.  Speech with a short pitch period (e.g. female) has less time to decay between pitch cycles so is less sensitive to formant bandwidths, the short pitch period naturally leads to a more uniform distribution of energy over time.

At the Codec 2 receiver, a post filter $P()$ is applied which improves the perceptual quality, in particular for low pitched (male) speakers. The post filter is a form of ad-hoc non-linear filtering, using experimentally derived constants.  It raises formants and lowers anti-formants, effectively reducing formant bandwidth and more widely distributing energy in time (each formant ``rings" longer).

Neural codecs have shown high quality speech can be synthesised using vectors of sparsely sampled frequency domain samples (MFCCs) \cite{davis1980comparison}.  Applying linear transformations (such as IDCT and interpolation) to recover $\{A_m\}$ from MFCCs results in poor quality synthesised speech in sinusoidal codecs.  This implies neural codecs are employing a non-linear transformation, made possible by modern deep learning techniques.

From an information theory perspective there is no reason to believe there is more information in speech from low pitch speakers than high pitched speakers. Therefore with an appropriate transformation we should be able to synthesise equivalent quality speech regardless of pitch.

This document describes two experiments to determine if narrow bandwidth formants can be preserved for low pitch speakers using non-linear transformations, resulting in reasonable quality speech when synthesised using Codec 2 (without the use of a post filter).  

\section{Vector Quantisation}
\label{sec:vq}

Vector quantisers can take advantage of linear and non-linear dependencies to reduce bit rate \cite{makhoul1985vector}.  They can efficiently exploit information that is uncorrelated in a linear sense (no linear transformation exists), but is statistically dependant.

The 700C resampling to a fixed length $K<<L_{max}$ is a linear transformation aimed at fixing the dimension of the data; reducing the distortion with multistage VQ where residuals tend towards independent Gaussians; and reducing VQ storage requirements.  However the resampling is a filtering/smoothing/aliasing operation, so some information is lost, for example narrow bandwidth formants cannot be recovered by a linear transformation.

In this experiment we will upsample the variable rate $L$ vectors to a fixed length $K=80$, then vector quantise.  In this way no information is lost from the source vectors.  The aim is to utilise the non-linear dependency matching properties of VQ to preserve high quality speech, while still maintaining a similar bit rate to $K=20$ quantisation.  We will ignore storage concerns for this experiment. 

\section{Machine Learning}

In this approach we will attempt to take a $K=20$ vector and resample it to $K=80$ using a small neural network.   We hope the network will discover any non-linear dependencies, and produce narrow bandwidth formants and (when synthesised using Codec 2) reasonable quality speech for low pitched speakers.  

We will include pitch as a feature (perhaps via an embedding network), arguing that the formant bandwidth is a function of pitch ($F_0$).  This is information the VQ in Section \ref{sec:vq} does not consider, which implies better expected results from the ML approach. Using a fixed length target vector simplifies any issues around variable vector length.  This is similar to the decoder side of an autoencoder network.  Autoencoder designs could therefore be used as candidate architectures.

We argue that essentially the same information (in the form of MFCCs) is used for high quality speech synthesis with neural vocoders, therefore these networks must be performing a similar non-linear mapping of coarsely sampled, smoothed spectral information to speech spectra that includes narrow bandwidth formants.  A counter argument is a simple neural network may not be capable of representing the non-linear function that maps between the two vectors.  However we are not trying to synthesise speech here - just the speech spectral envelope.  Other differences: the network proposed here outputs linear values using regression, more sophisticated neural vocoders employ sophisticated conditional probability models and output a PDF; this network considers just one frame, rather than utilising an autoregressive design that considers many past samples.

\section{Evaluation}

Evaluation methods considered:
\begin{enumerate}
\item Informal listening tests and ranking across a small number of samples.
\item Evaluation of the results using objective measures such as Spectral Distortion (SD), however the relationship to subjective quality may be complex.  For a given bit rate SD may be larger at $K=80$ but the synthesised speech may sound better.
\item Another candidate objective measure is Peak Average Power Ratio (PAPR). This tends to be higher when the formant bandwidths are not preserved.
\item Visual inspection of speech spectra and waveforms of speech synthesised from both methods (prior to post filtering) would indicate if narrow formants have been preserved for males.
\end{enumerate}
	
Success would be indicated by evidence of formant bandwidth being preserved, and higher quality speech from male speakers compared to linear $K=20$ approaches. Speech should be synthesised using phases recovered from the output magnitude spectra using the minimum phase model - the degradation in quality is less obvious when original harmonic phases are used. 

For controls we could use:
\begin{enumerate}
\item Speech resampled through the $K=20$ "bottleneck" (without postfiltering).
\item Speech VQed at $K=20$ and $K=80$. 
\item Codec 2 3200 - this uses finely quantised LSPs that preserve formant shapes (and a post filter). 
\end{enumerate}

\section{Results}

$K=20$ and $K=80$ 2x12 bit (24 bit total VQs) were trained on 114,000 vectors using the LBG algorithm.  A single male and female sample were processed under a variety of conditions (see \path{README}).  The $K=80$ male sample sounded better than the $K=20$ sample, however there is still significant distortion.  The female sample was approximately the same for both $K=20$ and $K=80$.

Figure \ref{fig:big_dog_f61_k20} is a plot of a single frame from the male sample.  The SD was significantly higher for $K=80$ than $K=20$, the reverse of the subjective results.

Visual inspection of the time domain synthesised waveforms and measurement of PAPR showed no significant difference between the $K=20$ and $K=80$, VQ samples.

\begin{figure}
\caption{Voiced frame from male speaker, 24 bits/frame. Objective VQ distortion is higher at $K=80$ however formants are sharper and F3/F4 better preserved}
\label{fig:big_dog_f61_k20}
\begin{center}
\input big_dog_f61_k20.tex
\input big_dog_f61_k79.tex
\end{center}
\end{figure}

These experiments suggest the $K=80$ VQ is better at preserving perceptually important information at a given bit rate for the male sample tested.  A visual inspection  of the spectra suggests formants tend to get smoothed by the $K=20$ processing.  Therefore direct VQ of the $K=80$ vectors may simply be a better linear transformation (less smoothing, and/or preserving narrow formants through a higher sampling rate) than $K=20$ (downsampling, then followed by VQ). The results suggest there is a limit to how much downsampling is acceptable prior to VQ. The disadvantage of multi-stage VQ with large $K$ is the reduction in quantisation distortion is small from successive stages, as the residual vectors tend towards a set of $K$ independent Gaussians. There was no direct support for the energy distribution or non-linear dependency theory.  

\subsection{Notes and Further Work}

Notes:
\begin{enumerate}
\item Do we need an embedding layer for pitch?  Research embedding networks.
\item Should we test with some contrived data?  Introduce expected non-linearity and see if network can train as a check.
\item Just include voiced frames?  This seems like a good idea.  For UV frames pitch may not be meaningful, and may cause random noise. Set UV frames to a constant $F_0$ (like 0) so at least the network "knows".
\item Make sure we use energy normalisation \cite{rowe2023_ratek_study} when resampling to rate $K$.
\item Used mean removed vectors, just consider $200 < f < 3600$ Hz, for consistency with recent Codec 2 VQ work in \cite{rowe2023_ratek_study}.
\item The $K=80$ are linear spaced, $K=20$ Mel spaced.
\end{enumerate}

Further work:
\begin{enumerate}
\item If fixed $K$ works, try to come up with a network that handles sparse vectors.
\item Are MFCCs good choices for input features to neural vocoders? They are smoothed, linear transformations of the input speech.  Other latent spaces may be more suitable.
\item A linear transformation (like a DCT or PCA) followed by VQ is a good idea - truncating DCT coeffs will reduce the dimension, making storage easier, and VQ will capture any non-linear dependencies.  Smaller dimension will help residual error problem in multistage VQ. Trade off between truncation and VQ distortion.
\item Can we avoid gradient shift/tendancy towards Gaussians with multistage VQ?  Does mbest help?
\end{enumerate}

\bibliographystyle{plain}
\bibliography{manifold_refs}
\end{document}
